---
title: "Module 1 : Fondamentaux Business Intelligence"
toc: true
toc-depth: 3
number-sections: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Durée : 5 heures** | **Objectif : Maîtriser les concepts fondamentaux de la BI et comprendre l'architecture décisionnelle**

---

## 1.1 Introduction à la Business Intelligence : Enjeux et Définitions

### 1.1.1 Définition et Contexte

La **Business Intelligence** (BI) représente l'ensemble des technologies, processus et pratiques permettant de collecter, intégrer, analyser et présenter les données d'entreprise pour faciliter la prise de décision stratégique.

Dans un contexte où les organisations génèrent des volumes massifs de données, la BI transforme cette complexité en opportunités concrètes :

- **Visibilité opérationnelle** : comprendre ce qui se passe réellement dans l'entreprise
- **Anticipation stratégique** : détecter les tendances et prédire les évolutions
- **Optimisation des ressources** : identifier les inefficacités et les leviers d'amélioration
- **Avantage concurrentiel** : exploiter la donnée comme actif stratégique différenciant

### 1.1.2 Les Trois Piliers de la BI Moderne

```{mermaid}
graph TB
    A[Business Intelligence] --> B[Descriptive Analytics]
    A --> C[Predictive Analytics]
    A --> D[Prescriptive Analytics]
    
    B --> B1["Que s'est-il passé ?<br/>Tableaux de bord, Rapports"]
    C --> C1["Que va-t-il se passer ?<br/>Modèles prédictifs, Forecasting"]
    D --> D1["Que doit-on faire ?<br/>Recommandations, Optimisation"]
    
    style A fill:#2E86AB
    style B fill:#A23B72
    style C fill:#F18F01
    style D fill:#C73E1D
```

| Type d'Analytics | Objectif | Exemples d'Usage | Outils Typiques |
|-----------------|----------|------------------|-----------------|
| **Descriptive** | Comprendre le passé | Rapports de ventes mensuels, analyses de performance | Power BI, Tableau, Excel |
| **Diagnostic** | Expliquer les causes | Analyse des écarts budgétaires, root cause analysis | SQL, Python, DAX |
| **Predictive** | Anticiper l'avenir | Prévision des ventes, churn prediction | Python (scikit-learn), R, Azure ML |
| **Prescriptive** | Recommander des actions | Optimisation des prix, allocation de ressources | Algorithmes d'optimisation, IA |

### 1.1.3 Cas d'Usage Concrets par Département

**Finance & Contrôle de Gestion**


- Suivi budgétaire et analyse des écarts en temps réel 
- Consolidation financière multi-entités 
- Prévisions de trésorerie et cash-flow forecasting

**Commercial & Marketing**


- Analyse de la performance des ventes par segment/produit/région
- Scoring clients et segmentation RFM (Récence, Fréquence, Montant) 
- ROI des campagnes marketing et attribution multi-touch

**Opérations & Supply Chain**


- Optimisation des stocks et prévision de la demande
- Analyse de la performance logistique (taux de service, lead times)
- Suivi de la qualité et détection des anomalies

**Ressources Humaines**


- Tableaux de bord RH : turnover, absentéisme, productivité
- Analyse prédictive du départ des talents
- Planification des effectifs et gestion des compétences

---

## 1.2 Architecture Décisionnelle : De la Donnée à l'Insight

### 1.2.1 Vue d'Ensemble de l'Architecture BI

```{mermaid}
flowchart LR
    A[Sources de Données] --> B[ETL/ELT]
    B --> C[Data Warehouse]
    C --> D[Cube OLAP]
    D --> E[Couche Sémantique]
    E --> F[Reporting & Dashboards]
    
    A1[ERP<br/>SAP, Oracle] --> A
    A2[CRM<br/>Salesforce] --> A
    A3[Fichiers<br/>Excel, CSV] --> A
    A4[APIs<br/>Web Services] --> A
    A5[Bases de Données<br/>PostgreSQL, MySQL] --> A
    
    F --> F1[Power BI]
    F --> F2[Tableau]
    F --> F3[Excel]
    
    style C fill:#2E86AB
    style D fill:#A23B72
    style F fill:#F18F01
```

### 1.2.2 Les Composants de l'Architecture

#### **1. Sources de Données (Data Sources)**

Les données proviennent de multiples systèmes hétérogènes :

| Type de Source | Caractéristiques | Exemples | Challenges |
|---------------|------------------|----------|------------|
| **Systèmes transactionnels** | Données opérationnelles en temps réel | ERP (SAP, Oracle), CRM (Salesforce) | Volume élevé, structure complexe |
| **Fichiers plats** | Données semi-structurées | Excel, CSV, JSON, XML | Qualité variable, formats inconsistants |
| **Bases de données** | Données structurées | PostgreSQL, MySQL, SQL Server | Schémas différents, silos de données |
| **APIs & Web Services** | Données externes | REST APIs, réseaux sociaux, Open Data | Limitation de débit, authentification |
| **IoT & Capteurs** | Données streaming | Capteurs industriels, logs applicatifs | Volume massif, traitement temps réel |

#### **2. ETL/ELT : Extract, Transform, Load**

Le processus ETL constitue le cœur de l'intégration des données :

**ETL (Approche Traditionnelle)**
```{mermaid}
graph LR
    A[Extract<br/>Extraction] --> B[Transform<br/>Transformation]
    B --> C[Load<br/>Chargement]
    
    A1[Sources<br/>Hétérogènes] --> A
    C --> C1[Data<br/>Warehouse]
    
    style B fill:#F18F01
```

**Opérations de Transformation :**


- **Nettoyage** : suppression des doublons, gestion des valeurs manquantes
- **Normalisation** : uniformisation des formats (dates, devises, codes)
- **Enrichissement** : calcul de métriques dérivées, géocodage
- **Agrégation** : consolidation de données granulaires
- **Validation** : contrôles de cohérence et règles métier

**ELT (Approche Cloud Moderne)**


- Chargement des données brutes d'abord
- Transformation effectuée dans le Data Warehouse (utilise sa puissance de calcul)
- Plus adapté aux environnements Big Data et Cloud

**Outils ETL/ELT Populaires :**


- **Microsoft** : Azure Data Factory, SSIS (SQL Server Integration Services)
- **Open Source** : Talend, Apache NiFi, Airflow
- **Cloud** : AWS Glue, Google Dataflow, Fivetran

#### **3. Data Warehouse (Entrepôt de Données)**

Le Data Warehouse est le référentiel central optimisé pour l'analyse :

**Caractéristiques Clés :**

| Critère | OLTP (Transactionnel) | OLAP (Analytique) |
|---------|----------------------|-------------------|
| **Objectif** | Gérer les opérations quotidiennes | Analyser et décider |
| **Type de requêtes** | INSERT, UPDATE, DELETE | SELECT avec agrégations |
| **Volume de données** | Données courantes | Historique complet |
| **Normalisation** | Fortement normalisé (3NF) | Dénormalisé (modèle étoile) |
| **Performance** | Optimisé pour les transactions | Optimisé pour les lectures |
| **Utilisateurs** | Milliers d'opérateurs | Dizaines d'analystes |

**Architectures Data Warehouse :**

```{mermaid}
graph TB
    subgraph "Architecture Traditionnelle"
    A1[Sources] --> B1[Staging Area]
    B1 --> C1[Data Warehouse]
    C1 --> D1[Data Marts]
    end
    
    subgraph "Architecture Lakehouse Moderne"
    A2[Sources] --> B2[Data Lake<br/>Données brutes]
    B2 --> C2[Data Warehouse<br/>Données structurées]
    C2 --> D2[Couche Analytique]
    end
    
    style C1 fill:#2E86AB
    style C2 fill:#2E86AB
```

#### **4. Cubes OLAP (Online Analytical Processing)**

Les cubes OLAP permettent une analyse multidimensionnelle rapide :

**Opérations OLAP Fondamentales :**

```{mermaid}
graph LR
    A[Cube OLAP] --> B[Slice<br/>Découpage]
    A --> C[Dice<br/>Sous-cube]
    A --> D[Drill-Down<br/>Descente]
    A --> E[Drill-Up<br/>Montée]
    A --> F[Pivot<br/>Rotation]
    
    style A fill:#A23B72
```

| Opération | Description | Exemple |
|-----------|-------------|---------|
| **Slice** | Sélectionner une tranche du cube | Ventes pour l'année 2024 uniquement |
| **Dice** | Extraire un sous-cube | Ventes 2024 pour la France et l'Allemagne |
| **Drill-Down** | Descendre dans la hiérarchie | De l'année → trimestre → mois → jour |
| **Drill-Up** | Remonter dans la hiérarchie | Du produit → catégorie → division |
| **Pivot** | Faire pivoter les axes d'analyse | Permuter lignes et colonnes |

**Types de Cubes OLAP :**


- **MOLAP** (Multidimensional) : stockage optimisé, très performant
- **ROLAP** (Relational) : basé sur des tables relationnelles
- **HOLAP** (Hybrid) : combinaison des deux approches

#### **5. Couche de Reporting & Visualisation**

La dernière couche transforme les données en insights visuels actionnables.

---

## 1.3 Cycle de Vie d'un Projet BI

### 1.3.1 Les Phases du Projet BI

```{mermaid}
gantt
    title Cycle de Vie Projet BI (6-9 mois)
    dateFormat  YYYY-MM-DD
    section Cadrage
    Expression des besoins      :a1, 2024-01-01, 30d
    Audit de l'existant         :a2, after a1, 15d
    
    section Conception
    Modélisation conceptuelle   :b1, after a2, 20d
    Architecture technique      :b2, after b1, 15d
    
    section Développement
    Développement ETL           :c1, after b2, 45d
    Création des cubes OLAP     :c2, after c1, 30d
    Développement dashboards    :c3, after c2, 30d
    
    section Déploiement
    Tests & Recette             :d1, after c3, 20d
    Formation utilisateurs      :d2, after d1, 10d
    Mise en production          :d3, after d2, 5d
    
    section Exploitation
    Support & Maintenance       :e1, after d3, 60d
```

### 1.3.2 Détail des Phases

#### **Phase 1 : Cadrage et Expression des Besoins (4-6 semaines)**

**Objectifs :**


- Définir le périmètre fonctionnel et les objectifs métier
- Identifier les parties prenantes et leurs besoins analytiques
- Évaluer la maturité BI de l'organisation

**Livrables :**


- Cahier des charges fonctionnel
- Matrice des KPI prioritaires
- Roadmap projet avec jalons
- Budget et planning prévisionnel

**Ateliers Métier :**

| Atelier | Participants | Durée | Objectif |
|---------|-------------|-------|----------|
| **Kickoff** | Sponsors, chef de projet | 2h | Aligner la vision et les enjeux |
| **Expression besoins** | Utilisateurs clés, métier | 3h x 3 | Recueillir les besoins analytiques détaillés |
| **Priorisation KPI** | Comité de pilotage | 2h | Définir les 15-20 KPI critiques |
| **Validation périmètre** | Direction, métier | 2h | Valider le scope et le planning |

#### **Phase 2 : Conception de l'Architecture (4-5 semaines)**

**Modélisation Conceptuelle :**


- Identification des processus métier (ventes, achats, finance, etc.)
- Définition des dimensions et des faits
- Création du modèle logique de données (MLD)
- Documentation des règles de gestion

**Architecture Technique :**


- Choix des technologies (Power BI, Azure, SQL Server, etc.)
- Design de l'architecture ETL/ELT
- Stratégie de sécurité et gouvernance
- Plan de sauvegarde et de reprise d'activité

**Livrables :**


- Schémas d'architecture (conceptuel, logique, physique)
- Dictionnaire de données
- Dossier d'architecture technique (DAT)
- Matrice de traçabilité besoins/solutions

#### **Phase 3 : Développement (10-14 semaines)**

**Développement ETL (5-6 semaines) :**


- Extraction des données sources
- Implémentation des transformations
- Chargement dans le Data Warehouse
- Tests unitaires de chaque flux

**Création des Cubes OLAP (3-4 semaines) :**


- Implémentation du modèle en étoile
- Création des dimensions et hiérarchies
- Configuration des mesures et calculs
- Optimisation des agrégations

**Développement des Dashboards (4-5 semaines) :**


- Création des rapports Power BI / Tableau
- Développement des mesures DAX
- Design UX/UI des tableaux de bord
- Implémentation de la sécurité au niveau ligne (RLS)

#### **Phase 4 : Tests et Recette (3-4 semaines)**

**Tests Techniques :**


- Tests unitaires des flux ETL
- Tests d'intégration end-to-end
- Tests de performance et de charge
- Tests de sécurité et de conformité

**Recette Métier :**


- Validation fonctionnelle par les utilisateurs clés
- Contrôle de la cohérence des chiffres avec les systèmes sources
- Validation des règles de gestion
- Acceptation formelle des livrables

#### **Phase 5 : Déploiement et Formation (2-3 semaines)**

**Formation Utilisateurs :**

| Public | Contenu | Durée | Format |
|--------|---------|-------|--------|
| **Utilisateurs finaux** | Navigation, filtres, exports | 2h | Webinaire + documentation |
| **Power Users** | Création de rapports, DAX basique | 1 jour | Formation présentielle |
| **Administrateurs** | Gestion des modèles, sécurité | 2 jours | Formation technique |

**Mise en Production :**


- Déploiement de l'environnement de production
- Migration des données historiques
- Activation des flux automatisés
- Surveillance post-déploiement

#### **Phase 6 : Exploitation et Amélioration Continue**

**Support & Maintenance :**


- Support niveau 1 : assistance utilisateurs
- Support niveau 2 : correction d'anomalies
- Support niveau 3 : évolutions techniques

**Amélioration Continue :**


- Analyse de l'usage (nombre d'utilisateurs actifs, rapports consultés)
- Collecte du feedback utilisateurs
- Priorisation des évolutions
- Optimisation des performances

---

## 1.4 Panorama des Outils BI du Marché

### 1.4.1 Comparatif des Principales Plateformes

| Critère | Power BI | Tableau | Qlik Sense | Looker Studio |
|---------|----------|---------|------------|---------------|
| **Éditeur** | Microsoft | Salesforce | Qlik | Google |
| **Courbe d'apprentissage** | ⭐⭐⭐ Modérée | ⭐⭐⭐⭐ Facile | ⭐⭐ Complexe | ⭐⭐⭐⭐⭐ Très facile |
| **Puissance analytique** | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ |
| **Intégration écosystème** | Excellent (Microsoft) | Bon (Salesforce) | Bon | Excellent (Google) |
| **Prix (par utilisateur/mois)** | 10-20€ (Pro/Premium) | 70-140$ | 30-100$ | Gratuit - 10€ |
| **Déploiement** | Cloud ou On-premise | Cloud ou On-premise | Cloud ou On-premise | Cloud uniquement |
| **Langage de calcul** | DAX | Calculs Tableau | Qlik Script | SQL-like |
| **Connecteurs natifs** | 200+ | 100+ | 150+ | 800+ (via Google) |
| **Mobile** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |
| **Collaboration** | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ |

### 1.4.2 Microsoft Power BI : L'Écosystème Complet

```{mermaid}
graph TB
    A[Power BI Ecosystem] --> B[Power BI Desktop]
    A --> C[Power BI Service]
    A --> D[Power BI Mobile]
    A --> E[Power BI Embedded]
    A --> F[Power BI Report Server]
    
    B --> B1["Développement de rapports<br/>Modélisation de données<br/>DAX & Power Query"]
    C --> C1["Publication & Partage<br/>Collaboration<br/>Planification de rafraîchissement"]
    D --> D1["Consultation mobile<br/>Notifications<br/>Mode offline"]
    E --> E1["Intégration dans<br/>applications tierces<br/>White-labeling"]
    F --> F1["Déploiement<br/>on-premise<br/>Pour organisations isolées"]
    
    style A fill:#2E86AB
    style B fill:#F18F01
    style C fill:#A23B72
```

**Avantages Power BI :**


- ✅ Intégration transparente avec l'écosystème Microsoft (Excel, Azure, Office 365)
- ✅ Rapport qualité/prix exceptionnel
- ✅ DAX : langage puissant pour les calculs complexes
- ✅ Mises à jour mensuelles avec nouvelles fonctionnalités
- ✅ Large communauté et nombreuses ressources de formation

**Limitations :**


- ❌ Dépendance à l'écosystème Microsoft
- ❌ Personnalisation visuelle limitée comparée à Tableau
- ❌ Performance sur très gros volumes (> 10 Go)

### 1.4.3 Tableau : L'Excellence de la Visualisation

**Points Forts :**


- ✅ Interface intuitive et ergonomique
- ✅ Capacités de visualisation avancées et flexibles
- ✅ Excellente gestion des données spatiales (cartographie)
- ✅ Communauté très active (Tableau Public)

**Cas d'Usage Privilégiés :**


- Dashboards marketing et sales
- Analyses géospatiales
- Storytelling avec données
- Explorations ad-hoc

### 1.4.4 Qlik Sense : Le Moteur Associatif

**Innovation Clé : Moteur Associatif**

Contrairement aux autres outils, Qlik ne filtre pas les données mais maintient les associations entre toutes les valeurs, permettant une exploration plus intuitive.

**Avantages :**


- ✅ Exploration libre et découverte de relations cachées
- ✅ Performance élevée même sur gros volumes
- ✅ Sécurité granulaire et gouvernance robuste

### 1.4.5 Looker Studio : L'Outil Accessible de Google

**Avantages :**


- ✅ Gratuit pour usage de base
- ✅ Intégration native avec Google Analytics, Ads, BigQuery
- ✅ Collaboration en temps réel (comme Google Docs)
- ✅ Aucune installation requise

**Limitations :**


- ❌ Capacités analytiques limitées
- ❌ Performance sur gros volumes
- ❌ Moins de connecteurs vers systèmes d'entreprise

---

## 1.5 Gouvernance et Culture Data-Driven

### 1.5.1 Les Piliers de la Gouvernance des Données

```{mermaid}
graph TB
    A[Gouvernance des Données] --> B[Qualité des Données]
    A --> C[Sécurité & Confidentialité]
    A --> D[Métadonnées & Documentation]
    A --> E[Rôles & Responsabilités]
    A --> F[Processus & Standards]
    
    B --> B1["Exactitude<br/>Complétude<br/>Cohérence<br/>Actualité"]
    C --> C1["RGPD<br/>Contrôle d'accès<br/>Chiffrement<br/>Audit"]
    D --> D1["Dictionnaire de données<br/>Lignage<br/>Catalogue de données"]
    E --> E1["Data Owners<br/>Data Stewards<br/>Data Architects"]
    F --> F1["Conventions de nommage<br/>Processus ETL<br/>Cycle de vie"]
    
    style A fill:#2E86AB
```

### 1.5.2 Le Framework de Qualité des Données

**Les 6 Dimensions de la Qualité :**

| Dimension | Définition | Mesure | Exemple d'Anomalie |
|-----------|------------|--------|-------------------|
| **Exactitude** | Les données reflètent la réalité | % de conformité aux règles | Adresse email invalide |
| **Complétude** | Absence de valeurs manquantes | % de champs remplis | Code postal manquant |
| **Cohérence** | Uniformité entre sources | % de concordance | Client avec 2 dates de naissance différentes |
| **Actualité** | Fraîcheur des données | Délai de mise à jour | Stock non actualisé depuis 3 jours |
| **Unicité** | Absence de doublons | % de doublons détectés | 3 comptes pour le même client |
| **Validité** | Respect des formats et contraintes | % de valeurs valides | Date future dans un champ historique |

**Processus de Data Quality Management :**

```{mermaid}
flowchart LR
    A[Profiling<br/>Analyse initiale] --> B[Validation<br/>Règles de qualité]
    B --> C[Nettoyage<br/>Correction]
    C --> D[Enrichissement<br/>Complétion]
    D --> E[Monitoring<br/>Surveillance]
    E --> A
    
    style A fill:#F18F01
    style C fill:#2E86AB
    style E fill:#A23B72
```

### 1.5.3 Sécurité et Conformité RGPD

**Principes de Sécurité BI :**

1. **Authentification** : qui accède aux données ?
2. **Autorisation** : qui peut voir quoi ?
3. **Audit** : traçabilité des accès et modifications
4. **Chiffrement** : protection des données sensibles
5. **Anonymisation** : masquage des données personnelles

**Row-Level Security (RLS) : Exemple Power BI**

Scénario : Les directeurs régionaux ne doivent voir que les données de leur région.

```dax
-- Exemple de filtre RLS dans Power BI
[Région] = USERPRINCIPALNAME()

-- Ou plus sophistiqué avec table de mapping
VAR UserEmail = USERPRINCIPALNAME()
VAR UserRegion = LOOKUPVALUE(
    Users[Région],
    Users[Email], UserEmail
)
RETURN [Région] = UserRegion
```

**Conformité RGPD : Checklist BI**

- [ ] Inventaire des données personnelles traitées
- [ ] Finalité légitime et transparente
- [ ] Minimisation des données collectées
- [ ] Durée de conservation définie
- [ ] Droit d'accès et de rectification implémenté
- [ ] Droit à l'oubli (suppression sur demande)
- [ ] Chiffrement des données sensibles
- [ ] Registre des traitements tenu à jour
- [ ] DPO (Data Protection Officer) désigné
- [ ] Analyse d'impact (PIA) réalisée si nécessaire

### 1.5.4 Construire une Culture Data-Driven

**Les 5 Niveaux de Maturité Data**

```{mermaid}
graph LR
    A[Niveau 1<br/>Ad-hoc] --> B[Niveau 2<br/>Réactif]
    B --> C[Niveau 3<br/>Défini]
    C --> D[Niveau 4<br/>Géré]
    D --> E[Niveau 5<br/>Optimisé]
    
    style A fill:#C73E1D
    style C fill:#F18F01
    style E fill:#06A77D
```

| Niveau | Caractéristiques | Actions Clés |
|--------|------------------|--------------|
| **1. Ad-hoc** | Données en silos, décisions intuitives | Sensibiliser la direction |
| **2. Réactif** | Rapports manuels, analyses ponctuelles | Lancer premiers projets BI |
| **3. Défini** | Processus standardisés, BI centralisée | Former les équipes, gouvernance |
| **4. Géré** | Mesure de la performance, dashboards temps réel | Self-service BI, culture data |
| **5. Optimisé** | IA/ML intégrés, amélioration continue | Innovation data, data products |

**Facteurs Clés de Succès :**

1. **Sponsorship exécutif** : engagement visible de la direction
2. **Quick wins** : projets pilotes rapides pour démontrer la valeur
3. **Formation continue** : montée en compétence des équipes
4. **Démocratisation** : self-service BI pour les métiers
5. **Mesure de l'impact** : ROI des initiatives data

---

## 1.6 Atelier Pratique : Cartographie de l'Écosystème Data

### 1.6.1 Objectifs de l'Atelier

Cet atelier de 90 minutes permet aux participants de :


- Cartographier l'architecture data d'une entreprise fictive
- Identifier les sources de données et leurs flux
- Définir 5 KPI stratégiques prioritaires
- Proposer une roadmap BI simplifiée

### 1.6.2 Cas d'Étude : TechnoRetail SA

**Contexte Entreprise :**

TechnoRetail est une chaîne de magasins d'électronique avec :


- 50 points de vente en France
- 200M€ de CA annuel
- E-commerce représentant 30% du CA
- 500 000 clients actifs

**Problématiques Métier :**


- Visibilité limitée sur la performance magasin vs web
- Ruptures de stock fréquentes
- Difficulté à analyser le comportement client omnicanal
- Reporting financier manuel et chronophage

### 1.6.3 Cartographie des Sources de Données

**Exercice 1 : Identifier les sources (15 min)**

Les participants doivent lister et catégoriser les sources de données :

| Système | Type | Données Clés | Fréquence de Mise à Jour |
|---------|------|--------------|--------------------------|
| **SAP ECC** | ERP | Ventes, achats, stocks, comptabilité | Temps réel |
| **Salesforce** | CRM | Clients, contacts, opportunités | Temps réel |
| **Magento** | E-commerce | Commandes web, paniers abandonnés | Temps réel |
| **Google Analytics** | Web analytics | Trafic, conversions, parcours | Batch quotidien |
| **Sage Paie** | RH | Effectifs, salaires, absences | Mensuel |
| **Fichiers Excel** | Fichiers | Budgets, prévisions | Mensuel |

**Exercice 2 : Dessiner l'architecture cible (20 min)**

```{mermaid}
graph TB
    subgraph Sources
        A1[SAP ECC]
        A2[Salesforce]
        A3[Magento]
        A4[Google Analytics]
        A5[Fichiers Excel]
    end
    
    subgraph ETL
        B[Azure Data Factory]
    end
    
    subgraph Storage
        C1[Data Lake<br/>Azure Data Lake]
        C2[Data Warehouse<br/>Azure Synapse]
    end
    
    subgraph Semantic
        D[Modèle Power BI<br/>Étoile Ventes]
    end
    
    subgraph Reporting
        E1[Dashboards Ventes]
        E2[Rapports Financiers]
        E3[Analytics Clients]
    end
    
    A1 --> B
    A2 --> B
    A3 --> B
    A4 --> B
    A5 --> B
    
    B --> C1
    C1 --> C2
    C2 --> D
    D --> E1
    D --> E2
    D --> E3
    
    style C2 fill:#2E86AB
    style D fill:#F18F01
```

### 1.6.4 Définition des KPI Stratégiques

**Exercice 3 : Identifier 5 KPI prioritaires (25 min)**

Méthodologie SMART pour définir des KPI efficaces :


- **S**pécifique : clairement défini et compréhensible
- **M**esurable : quantifiable avec une unité de mesure
- **A**tteignable : réaliste et actionnable
- **R**elevant : aligné avec les objectifs stratégiques
- **T**emporel : avec une fréquence de mise à jour définie

**Template de Fiche KPI :**

| Élément | Description |
|---------|-------------|
| **Nom du KPI** | Nom explicite |
| **Définition** | Formule de calcul détaillée |
| **Objectif métier** | Pourquoi mesurer ce KPI ? |
| **Source de données** | Système(s) source(s) |
| **Fréquence de mise à jour** | Temps réel, quotidien, hebdomadaire, mensuel |
| **Propriétaire** | Responsable métier |
| **Seuil d'alerte** | Valeurs limites (min/max) |
| **Visualisation recommandée** | Type de graphique |

**Exemple de 5 KPI Stratégiques pour TechnoRetail :**

#### KPI 1 : Chiffre d'Affaires Consolidé

```dax
-- ===== SALES REVENUE KPIs =====

Total Sales = 
SUM(FACT_Sales[SalesAmount])

Store Sales = 
CALCULATE(
    [Total Sales],
    'DIM_Channel'[ChannelName] = "Store"
)

Web Sales = 
CALCULATE(
    [Total Sales],
    'DIM_Channel'[ChannelName] = "Web"
)

Mobile Sales = 
CALCULATE(
    [Total Sales],
    'DIM_Channel'[ChannelName] = "Mobile"
)

Sales YoY % = 
IF(
    HASONEVALUE('DIM_Date'[Year]),
    VAR CurrentSales = [Total Sales]
    VAR CurrentYear = MAX('DIM_Date'[Year])
    VAR PreviousYearSales =
        CALCULATE(
            [Total Sales],
            FILTER(
                ALL('DIM_Date'),
                'DIM_Date'[Year] = CurrentYear - 1
            )
        )
    RETURN
        IF(
            ISBLANK(PreviousYearSales) || PreviousYearSales = 0,
            0,
            DIVIDE(CurrentSales - PreviousYearSales, PreviousYearSales)
        ),
    BLANK()  -- Retourne vide pour le total
)
```

| Critère | Détail |
|---------|--------|
| **Formule** | Somme des montants HT de toutes les ventes |
| **Objectif** | Suivre la performance commerciale globale |
| **Source** | SAP ECC + Magento |
| **Fréquence** | Temps réel |
| **Seuil** | Objectif annuel : 220M€ (+10% vs N-1) |
| **Visualisation** | Carte KPI + Line chart avec tendance |

#### KPI 2 : Taux de Rupture de Stock

```dax
Stock Out Rate = 
VAR OutOfStockDays = 
    CALCULATE(
        COUNTROWS('FACT_Stock'),
        'FACT_Stock'[QuantityInStock] = 0
    )
VAR TotalDays = 
    COUNTROWS('FACT_Stock')
RETURN
    DIVIDE(OutOfStockDays, TotalDays, 0)

Out of Stock Products = 
CALCULATE(
    DISTINCTCOUNT('FACT_Stock'[ProductKey]),
    'FACT_Stock'[QuantityInStock] = 0
)
```

| Critère | Détail |
|---------|--------|
| **Formule** | (Nb jours en rupture / Nb jours total) × 100 |
| **Objectif** | Optimiser la disponibilité produits |
| **Source** | SAP ECC (module MM) |
| **Fréquence** | Quotidien |
| **Seuil** | < 5% (alerte si > 8%) |
| **Visualisation** | Gauge + Table des produits critiques |

#### KPI 3 : Panier Moyen Omnicanal

```dax
-- ===== AVERAGE BASKET KPIs =====

Average Basket = 
DIVIDE(
    [Total Sales],
    DISTINCTCOUNT('FACT_Sales'[TransactionID]),
    0
)

Store Basket = 
CALCULATE(
    [Average Basket],
    'DIM_Channel'[ChannelName] = "Store"
)

Web Basket = 
CALCULATE(
    [Average Basket],
    'DIM_Channel'[ChannelName] = "Web"
)

Mobile Basket = 
CALCULATE(
    [Average Basket],
    'DIM_Channel'[ChannelName] = "Mobile"
)

Basket Evolution = 
[Average Basket] - CALCULATE([Average Basket], PREVIOUSMONTH('DIM_Date'[FullDate]))
```

| Critère | Détail |
|---------|--------|
| **Formule** | CA total / Nombre de transactions |
| **Objectif** | Mesurer l'efficacité commerciale |
| **Source** | SAP ECC + Magento |
| **Fréquence** | Hebdomadaire |
| **Seuil** | Objectif : 85€ (actuellement 78€) |
| **Visualisation** | KPI Card + Comparaison Web vs Magasin |

#### KPI 4 : Taux de Conversion Web

```dax
Web Conversion Rate = 
VAR Sessions = SUM('FACT_Analytics'[Sessions])
VAR Transactions = DISTINCTCOUNT('FACT_Sales'[TransactionID])
RETURN
    DIVIDE(Transactions, Sessions, 0)

Abandoned Carts = 
CALCULATE(
    COUNTROWS('FACT_Analytics'),
    'FACT_Analytics'[Event] = "Cart Abandonment"
)

Cart Abandonment Rate = 
DIVIDE([Abandoned Carts], [Abandoned Carts] + [Total Transactions], 0)
```

| Critère | Détail |
|---------|--------|
| **Formule** | (Transactions web / Sessions) × 100 |
| **Objectif** | Optimiser le tunnel de conversion e-commerce |
| **Source** | Google Analytics + Magento |
| **Fréquence** | Temps réel |
| **Seuil** | Objectif : 3.5% (actuellement 2.8%) |
| **Visualisation** | Funnel chart + Analyse des abandons |

#### KPI 5 : Net Promoter Score (NPS)

```dax
NPS = 
VAR Promoters = 
    CALCULATE(
        COUNTROWS('FACT_Satisfaction'),
        'FACT_Satisfaction'[Rating] >= 9
    )
VAR Detractors = 
    CALCULATE(
        COUNTROWS('FACT_Satisfaction'),
        'FACT_Satisfaction'[Rating] <= 6
    )
VAR Total = COUNTROWS('FACT_Satisfaction')
RETURN
    DIVIDE(Promoters - Detractors, Total, 0) * 100

NPS by Channel = 
CALCULATE(
    [NPS],
    ALLEXCEPT('FACT_Satisfaction', 'FACT_Satisfaction'[Channel])
)
```

| Critère | Détail |
|---------|--------|
| **Formule** | % Promoteurs (9-10) - % Détracteurs (0-6) |
| **Objectif** | Mesurer la satisfaction et fidélité client |
| **Source** | Salesforce (enquêtes post-achat) |
| **Fréquence** | Mensuel |
| **Seuil** | Objectif : +40 (actuellement +28) |
| **Visualisation** | Gauge + Distribution des notes |

### 1.6.5 Dashboard de Synthèse : Mockup

**Exercice 4 : Esquisser le dashboard exécutif (30 min)**

Les participants travaillent en groupe pour concevoir un dashboard de synthèse intégrant les 5 KPI.

**Principes de Design :**

| Principe | Application |
|----------|-------------|
| **Hiérarchie visuelle** | Les KPI critiques en haut, détails en bas |
| **Règle des 5 secondes** | Le message clé doit être compris en 5 secondes |
| **Consistance** | Même palette de couleurs, même format de chiffres |
| **Interactivité** | Filtres dynamiques (période, région, canal) |
| **Contexte** | Toujours comparer (vs objectif, vs N-1, vs benchmark) |

**Structure Recommandée :**

```{mermaid}
graph TB
    A[En-tête : Titre + Filtres] --> B[Zone KPI : 5 Cartes Principales]
    B --> C[Zone Tendances : Graphiques Temporels]
    C --> D[Zone Analyse : Détails par Dimension]
    
    style A fill:#2E86AB
    style B fill:#F18F01
    style C fill:#A23B72
    style D fill:#06A77D
```

---

## 1.7 Synthèse du Module 1 et Points Clés

### 1.7.1 Récapitulatif des Concepts

À l'issue de ce module, les participants maîtrisent :


✅ **Les fondamentaux de la BI** : définitions, enjeux, niveaux d'analytics  
✅ **L'architecture décisionnelle** : de la source au dashboard  
✅ **Le cycle de vie projet BI** : phases, livrables, parties prenantes  
✅ **Le panorama des outils** : Power BI, Tableau, Qlik, Looker Studio  
✅ **La gouvernance et la qualité des données** : RGPD, sécurité, culture data-driven  
✅ **La méthodologie de définition des KPI** : approche SMART, dashboarding

### 1.7.2 Checklist de Fin de Module

**Auto-évaluation des Acquis :**

- [ ] Je peux expliquer la différence entre OLTP et OLAP
- [ ] Je comprends le rôle de chaque composant de l'architecture BI
- [ ] Je sais définir un KPI selon la méthodologie SMART
- [ ] Je peux comparer les avantages de Power BI vs Tableau
- [ ] Je connais les 6 dimensions de la qualité des données
- [ ] Je peux identifier les phases d'un projet BI
- [ ] Je comprends les enjeux de la gouvernance et du RGPD

### 1.7.3 Ressources Complémentaires

**Lectures Recommandées :**


- **Livres** : 
  - "The Data Warehouse Toolkit" - Ralph Kimball
  - "Building a Data-Driven Organization" - Carl Anderson
  - "Storytelling with Data" - Cole Nussbaumer Knaflic
  
**Sites Web & Communautés :**


- [Microsoft Learn - Power BI](https://learn.microsoft.com/power-bi/)
- [Tableau Community Forums](https://community.tableau.com/)
- [DAMA International](https://www.dama.org/) - Data Management Body of Knowledge
  
**Certifications :**


- Microsoft Certified: Power BI Data Analyst Associate (PL-300)
- Tableau Desktop Specialist
- AWS Certified Data Analytics - Specialty

---